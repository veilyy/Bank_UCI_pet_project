#  Оптимизация банковского маркетинга: Baseline ML-модель

**Задача:** Предсказать, откроет ли клиент депозит после
телефонного звонка  
**Цель:** Увеличить конверсию звонков через ML-приоритизацию клиентов  
**Датасет:** UCI Bank Marketing (41k записей, дисбаланс 88%/12%)  
**Модель:** Логистическая регрессия с оптимизированным порогом  

# Метрики модели

| Метрика | Значение 
|---------|----------
| **Precision** | 0.455 
| **Recall** | 0.571 
| **F1-Score** | 0.507 
| **ROC-AUC** | 0.801
| **Accuracy** | 0.875 

**Порог:** 0.67 (найден через анализ F1-score)

## Техническая реализация

### Пайплайн препроцессинга:
```python
1. Кодирование цели: {'yes': 1, 'no': 0}
2. Удаление 'duration' (признак известен только после звонка)
3. Обработка редких категорий
4. One-Hot Encoding (10 категориальных признаков)
5. StandardScaler (9 числовых признаков)
6. Стратифицированное разделение 80/20

Конфигурация модели

LogisticRegression(
    class_weight='balanced',  # Учет дисбаланса классов
    random_state=42,         
    max_iter=1000,            
    solver='lbfgs',           
    C=1.0                     
)
```
# Анализ результатов
⚠️ Низкий Precision (0.455) - много ложных срабатываний

⚠️ Дисбаланс классов (88%/12%) искажает метрики

⚠️ Линейная модель не улавливает нелинейные зависимости

⚠️ Feature engineering не проводился - только базовый препроцессинг

# План улучшений
## Feature Preprocessing
1. Создание новых признаков: некоторые экономические признаки сильно коррелируют, что плохо сказывается на эффективности модели, создадим одну информативную фичу на их основе.

2. Отбор признаков: Уберем малоинформативные признаки (на основе их распределения и анализируя feature importance бейзлайна)

## Эксперименты с алгоритмами
1. Random Forest Classifier

   • Плюсы: ловит нелинейности

   • Минусы: может переобучиться

2. Gradient Boosting (XGBoost)

   • Плюсы: один из лучших алгоритмов для табличных данных

   • Минусы: требует настройки гиперпараметров